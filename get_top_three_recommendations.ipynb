{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 14)  # Assuming 10 classes\n",
    "    return model\n",
    "# Function to load a model and its state dict from a .pth file\n",
    "def load_model1(model_path):\n",
    "    model = create_model1()  # Create the model architecture\n",
    "    state_dict = torch.load(model_path)  # Load the state dict\n",
    "    model.load_state_dict(state_dict)  # Load the state dict into the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "def create_model2():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 47)  # Assuming 10 classes\n",
    "    return model\n",
    "def load_model2(model_path):\n",
    "    model = create_model2()  # Create the model architecture\n",
    "    state_dict = torch.load(model_path)  # Load the state dict\n",
    "    model.load_state_dict(state_dict)  # Load the state dict into the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "def create_model3():\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 6)  # Assuming 10 classes\n",
    "    return model\n",
    "def load_model3(model_path):\n",
    "    model = create_model3()  # Create the model architecture\n",
    "    state_dict = torch.load(model_path)  # Load the state dict\n",
    "    model.load_state_dict(state_dict)  # Load the state dict into the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def preprocess_image(image_path):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = preprocess(img)\n",
    "    img = img.unsqueeze(0) # Add batch dimension\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_models():\n",
    "    # Load the models\n",
    "    model_paths = ['/newstorage5/aayesha/deep_learning/classification/style_classification/resnet50_custom_style_multiclass_model_20Epochs.pth', '/newstorage5/aayesha/deep_learning/classification/pattern_classification/resnet50_custom_pattern_multiclass_model_20Epochs.pth', '/newstorage5/aayesha/deep_learning/classification/fabric_classification/resnet50_custom_fabric_multiclass_model_20Epochs.pth']\n",
    "    #models = [load_model(model_path) for model_path in model_paths]\n",
    "    models = [load_model1(model_paths[0]), load_model2(model_paths[1]), load_model3(model_paths[2])]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image 1\n",
    "image_path = '/newstorage5/aayesha/deep_learning/classification/shirt1.jpg'\n",
    "input_image = preprocess_image(image_path)\n",
    "input_image1 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image 2\n",
    "image_path = '/newstorage5/aayesha/deep_learning/classification/shirt2.jpg'\n",
    "input_image = preprocess_image(image_path)\n",
    "input_image2 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image 3\n",
    "image_path = '/newstorage5/aayesha/red_round.jpg'\n",
    "input_image = preprocess_image(image_path)\n",
    "input_image3 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image 4\n",
    "image_path = '/newstorage5/aayesha/red_zigzag.jpg'\n",
    "input_image = preprocess_image(image_path)\n",
    "input_image4 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images = [input_image1, input_image2, input_image3, input_image4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference with each model and extract the feature vector\n",
    "def ext_feature_vec(input_image):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    outputs = []\n",
    "    all_classes = []\n",
    "    models = combine_models()\n",
    "    for model in models:\n",
    "        model = model.to(device)\n",
    "        with torch.no_grad():\n",
    "            # Assuming the models output the logits, features are extracted before the final classification layer\n",
    "            # Remove the final classification layer (e.g., if it's a fully connected layer in ResNet)\n",
    "            features = model(input_image)\n",
    "            outputs.append(features)\n",
    "            _, predicted = torch.max(features, 1)\n",
    "            x = predicted.item()\n",
    "        all_classes.append(x)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine model outputs into a single vector\n",
    "def combine_outputs_as_vector(outputs, method='concatenate'):\n",
    "    if method == 'concatenate':\n",
    "        combined_vector = torch.cat(outputs, dim=1)  # Concatenate along the feature dimension\n",
    "    elif method == 'average':\n",
    "        combined_vector = torch.mean(torch.stack(outputs), dim=0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid combination method provided.\")\n",
    "    return combined_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combined vector representation of all given images\n",
    "def get_all_vectors():\n",
    "    img_test = []\n",
    "    for j in range(len(input_images)):\n",
    "        outputs = ext_feature_vec(input_images[j])\n",
    "        combined_vector = combine_outputs_as_vector(outputs, method='concatenate')\n",
    "        img_test.append(combined_vector.cpu().numpy())\n",
    "    return img_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity between any two given vectors\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    vector1 = vector1.flatten()\n",
    "    vector2 = vector2.flatten()\n",
    "    cos_sim = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3617725/2464067799.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n",
      "/tmp/ipykernel_3617725/2464067799.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n",
      "/tmp/ipykernel_3617725/2464067799.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n"
     ]
    }
   ],
   "source": [
    "# Get similarities among images and return them\n",
    "def get_all_sim(input_images):\n",
    "    img_test = get_all_vectors()\n",
    "    all_sim = []\n",
    "    for k in range(len(img_test)):\n",
    "        each_sim = []\n",
    "        tracker = []\n",
    "        for m in range(len(img_test)):\n",
    "            if not np.array_equal(img_test[k], img_test[m]):\n",
    "                each_sim.append(cosine_similarity(img_test[k], img_test[m]))\n",
    "                tracker.append(m)       \n",
    "        maxi = max(each_sim)\n",
    "        idex_val = each_sim.index(maxi)\n",
    "        maxi_m = tracker[idex_val]\n",
    "        maxi_k = k\n",
    "        all_sim.append([maxi, maxi_m, maxi_k])\n",
    "    return [all_sim, img_test]\n",
    "all_sim = get_all_sim(input_images)[0]\n",
    "img_test = get_all_sim(input_images)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_items():\n",
    "    most_freq = []\n",
    "    for i in all_sim:\n",
    "        most_freq.append(i[1]) \n",
    "    return most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some calculations used for the later tasks\n",
    "\n",
    "def most_frequent_number(numbers):\n",
    "    # Create a Counter object to count the frequency of each number\n",
    "    counter = Counter(numbers)\n",
    "    # Get the most common number and its frequency\n",
    "    most_common_number, frequency = counter.most_common(1)[0]\n",
    "    return [most_common_number, frequency]\n",
    "\n",
    "def get_most_frequent_item_and_frequency():\n",
    "    numbers = get_most_similar_items()\n",
    "    most_frequent, count = most_frequent_number(numbers)\n",
    "    return [most_frequent, count]\n",
    "\n",
    "def find_indices(arr, target):\n",
    "    indices = []\n",
    "    for index, value in enumerate(arr):\n",
    "        if value == target:\n",
    "            indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_of_other_matching_items():\n",
    "    most_frequent = get_most_frequent_item_and_frequency()[0]\n",
    "    arr = get_most_similar_items()\n",
    "    target = most_frequent\n",
    "    rem_indices = find_indices(arr, target)\n",
    "    return rem_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_scores(all_sim):\n",
    "        indices = get_index_of_other_matching_items()\n",
    "        simi_val = []\n",
    "        for j in indices:               \n",
    "                simi_val.append([j, all_sim[j][0]])\n",
    "        return simi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_sim_scores(all_sim)\n",
    "def get_max_sim_score(x):\n",
    "    simi_arr = []\n",
    "    for each in x:\n",
    "        simi_arr.append(each[1])\n",
    "    max_score = max(simi_arr)\n",
    "    return([max_score, x[simi_arr.index(max_score)][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_sim_scores(all_sim)\n",
    "def get_max_score_item(x):\n",
    "    most_frequent = get_most_frequent_item_and_frequency()[0]\n",
    "    simi_arr = []\n",
    "    for each in x:\n",
    "        simi_arr.append(each[1])\n",
    "    max_score = max(simi_arr)\n",
    "    return([[max_score, most_frequent, x[simi_arr.index(max_score)][0]], [max_score, x[simi_arr.index(max_score)][0], most_frequent], most_frequent, x[simi_arr.index(max_score)][0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_removed_items():\n",
    "    removed_sim = get_max_score_item(x)[0:2]\n",
    "    return removed_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remain_sim_items():\n",
    "    top_sim = get_max_score_item(x)[0]\n",
    "    sec_sim = get_max_score_item(x)[1]\n",
    "    all_sim_rem= all_sim\n",
    "    all_sim_rem.remove(top_sim)\n",
    "    all_sim_rem.remove(sec_sim)\n",
    "    return all_sim_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_and_second_items():\n",
    "    top_item = get_max_score_item(x)[2]\n",
    "    second_item = get_max_score_item(x)[3]\n",
    "    return([top_item, second_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_third_item_sim_wrt_first_two(remain_sim):\n",
    "    top_item = top_and_second_items()[0]\n",
    "    second_item = top_and_second_items()[1]\n",
    "    #img_test = get_all_vectors()\n",
    "    remain_itm_id = []\n",
    "    #all_sim_rem = get_remain_sim_items()\n",
    "    for each in remain_sim:\n",
    "        remain_itm_id.append(each[2])\n",
    "    all_cos = []\n",
    "    for x in remain_itm_id:\n",
    "        cos1 = cosine_similarity(img_test[x], img_test[top_item])\n",
    "        cos2 = cosine_similarity(img_test[x], img_test[second_item])\n",
    "        all_cos.append([x, cos1 + cos2])\n",
    "    return all_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_third_place(remain_sim):\n",
    "   all_cos = get_third_item_sim_wrt_first_two(remain_sim)\n",
    "   third_place_item = max(all_cos, key=lambda x : x[1])\n",
    "   return third_place_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_three_items(remain_sim):\n",
    "    top_item = top_and_second_items()[0]\n",
    "    second_item = top_and_second_items()[1]\n",
    "    third_item = get_item_third_place(remain_sim)[0]\n",
    "    return (top_item, second_item, third_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entire_workflow_for_three_selections():\n",
    "    models = combine_models()\n",
    "    # Load and preprocess the input image\n",
    "    image_path = '/newstorage5/aayesha/deep_learning/classification/shirt1.jpg'\n",
    "    input_image = preprocess_image(image_path)\n",
    "    input_image1 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Load and preprocess the input image\n",
    "    image_path = '/newstorage5/aayesha/deep_learning/classification/shirt2.jpg'\n",
    "    input_image = preprocess_image(image_path)\n",
    "    input_image2 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Load and preprocess the input image\n",
    "    image_path = '/newstorage5/aayesha/red_round.jpg'\n",
    "    input_image = preprocess_image(image_path)\n",
    "    input_image3 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Load and preprocess the input image\n",
    "    image_path = '/newstorage5/aayesha/red_zigzag.jpg'\n",
    "    input_image = preprocess_image(image_path)\n",
    "    input_image4 = input_image.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_images = [input_image1, input_image2, input_image3, input_image4]\n",
    "    remain_sim = get_remain_sim_items()\n",
    "    top_three_recs = get_first_three_items(remain_sim)\n",
    "    return top_three_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3617725/2464067799.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n",
      "/tmp/ipykernel_3617725/2464067799.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n",
      "/tmp/ipykernel_3617725/2464067799.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path)  # Load the state dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "top_three_recs = entire_workflow_for_three_selections()\n",
    "print(top_three_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imgenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
